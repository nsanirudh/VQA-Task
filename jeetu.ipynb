{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21613,
     "status": "ok",
     "timestamp": 1542606419313,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "Gx32RFZ4cCml",
    "outputId": "bf1b2f54-f1d7-483f-8a7f-d7e7fd97709b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-272180d72b50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6936,
     "status": "ok",
     "timestamp": 1542606427870,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "d8HRZu8_cdKo",
    "outputId": "e481b0be-95bb-49dd-9e04-82f9dd90fdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is B6F7-1CD2\n",
      "\n",
      " Directory of C:\\Users\\Anirudh\\NNFLproj\n",
      "\n",
      "11/24/2018  12:12 AM    <DIR>          .\n",
      "11/24/2018  12:12 AM    <DIR>          ..\n",
      "11/24/2018  12:12 AM    <DIR>          .ipynb_checkpoints\n",
      "11/24/2018  12:12 AM            60,906 AllScratch(1).ipynb\n",
      "11/19/2018  11:51 PM    <DIR>          clevr-iep-master\n",
      "11/18/2018  07:20 PM    <DIR>          images\n",
      "11/04/2018  05:55 PM        21,006,448 Quest_Answers.json\n",
      "11/20/2018  12:36 AM                91 splitting.py\n",
      "11/18/2018  07:32 PM    11,867,228,256 train_features.h5\n",
      "11/19/2018  11:52 PM        25,928,032 train_questions.h5\n",
      "11/19/2018  11:52 PM             1,613 vocab.json\n",
      "               6 File(s) 11,914,225,346 bytes\n",
      "               5 Dir(s)  17,517,096,960 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6357,
     "status": "ok",
     "timestamp": 1542606427885,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "5tAfZUJ0cc_C",
    "outputId": "693e6da9-5494-461f-f590-0da0d062176b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive\n"
     ]
    }
   ],
   "source": [
    "cd gdrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1542606431587,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "Vd4jBOWkccrV",
    "outputId": "271ef640-309c-4f87-a510-ff4c1ca4944e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6915,
     "status": "ok",
     "timestamp": 1542606431595,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "wdzWU3Ckccg-",
    "outputId": "1026f648-f285-4e75-8ccd-a5c9a32b447f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive\n"
     ]
    }
   ],
   "source": [
    "cd My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11864,
     "status": "ok",
     "timestamp": 1542606436932,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "eO71ukmccvmN",
    "outputId": "3dc40309-6e63-4c9b-a601-f5572641c499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A1_csv.rar\n",
      " A1.pdf\n",
      "'Aviral Resume New.pdf'\n",
      "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
      "'Copy of Atention_of NNFL_Scratch.ipynb'\n",
      "\u001b[01;34m'Data Science Hackathon'\u001b[0m/\n",
      " \u001b[01;34mMNIST_Data\u001b[0m/\n",
      " model_weights.h5\n",
      "\u001b[01;34m'NNFL Lab1'\u001b[0m/\n",
      "\u001b[01;34m'NNFL Project'\u001b[0m/\n",
      " OUTING.docx\n",
      " \u001b[01;34msopdata\u001b[0m/\n",
      " \u001b[01;34mspamdata\u001b[0m/\n",
      "'TA Groups.xlsx'\n",
      "'TIMETABLE SEM II 2015-16 (11 JAN 2016).pdf'\n",
      " \u001b[01;34mtraining\u001b[0m/\n",
      " \u001b[01;34mTraining\u001b[0m/\n",
      " Training.zip\n",
      " Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11500,
     "status": "ok",
     "timestamp": 1542606436938,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "luFIt6WDc2xT",
    "outputId": "f5537b77-f990-4c28-e000-b1681705614f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/NNFL Project\n"
     ]
    }
   ],
   "source": [
    "cd NNFL\\ Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0sZa51jJc2kk"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIMsk-A9b0p-"
   },
   "source": [
    "### CREATING IMAGE PRE-MODEL (4096 OUTPUT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8vmU4BnCb0qR"
   },
   "outputs": [],
   "source": [
    "# from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# base_model = VGG16()\n",
    "# # print(base_model.summary())\n",
    "\n",
    "# # get output of penultimate layer\n",
    "# from keras.models import Model\n",
    "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uoXJwlFMb0qz"
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# # # load an image from file\n",
    "# # image = load_img('Coffee-Mug.jpg', target_size=(224, 224))\n",
    "# # # convert the image pixels to a numpy array\n",
    "# # image = img_to_array(image)\n",
    "# # # reshape data for the model\n",
    "# # image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# # # prepare the image for the VGG model\n",
    "# # image = preprocess_input(image)\n",
    "\n",
    "# # # predict the probability across all output classes\n",
    "# # yhat = model.predict(image)\n",
    "# # print(yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7cbYqXqb0rG"
   },
   "source": [
    "### MAKE IMAGE DATA DICT ###\n",
    "##### key: image name\n",
    "##### value: vector of shape [1,4096], output of VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ua5zAKxVb0rO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e9d7bce5596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# imagedictv2: Images of size 100x100\n",
    "\n",
    "if(os.path.isfile('image_dict.pickle')):\n",
    "    with open('image_dict.pickle', 'rb') as handle:\n",
    "        image_dict = pickle.load(handle)\n",
    "        \n",
    "# else:\n",
    "#   folder = 'Training/images/'\n",
    "\n",
    "#   image_dict_V2 = {}        \n",
    "\n",
    "#   skipped = []\n",
    "#   i=0\n",
    "\n",
    "#   for filename in os.listdir(folder):\n",
    "#       if(i%50 == 0):\n",
    "#           print(\"{} images processed\".format(i))\n",
    "#       i+=1\n",
    "#       if filename in image_dict_V2:\n",
    "#           # print (\"already in dict - moving on\")\n",
    "#           continue\n",
    "#       try:\n",
    "#           # load an image from file\n",
    "#           image = cv2.imread(os.path.join(folder, filename))\n",
    "#       except:\n",
    "#           print(\"Error reading file: {}!!!\".format(filename))\n",
    "#           skipped.append(filename)\n",
    "#           continue\n",
    "#       if image is not None:\n",
    "#           resized_image = cv2.resize(image, (100, 100)) \n",
    "#           image_dict_V2[filename] = resized_image\n",
    "#       else:\n",
    "#           skipped.append(filename)\n",
    "\n",
    "#   print(\"{} files skipped:\".format(len(skipped)))\n",
    "#   for f in skipped:\n",
    "#       print(\"    {}\".format(f))\n",
    "#   print(\"dict created\")\n",
    "\n",
    "\n",
    "#   print(\"Saving image_dict_V2.pickle\")\n",
    "#   with open('image_dict_V2.pickle', 'wb') as handle:\n",
    "#       pickle.dump(image_dict_V2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#       print('image_dict_V2.pickle saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IyPHry89b0rg"
   },
   "outputs": [],
   "source": [
    "# # Save dict pickle\n",
    "# import pickle\n",
    "\n",
    "# with open('image_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(images, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     print('image_dict.pickle saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJzDSB7Ob0ru"
   },
   "source": [
    "### COMBINING MODELS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qIMeF19ob0r0"
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "# def img_model(dropout_rate):\n",
    "#     print(\"Creating image model...\")\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "#     return model\n",
    "\n",
    "# def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "#     print(\"Creating text model...\")\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(num_words, embedding_dim, \n",
    "#         weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "#     model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(LSTM(units=512, return_sequences=False))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1024, activation='tanh'))\n",
    "#     return model\n",
    "\n",
    "# def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "#     vgg_model = img_model(dropout_rate)\n",
    "#     lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "#     print(\"Merging final model...\")\n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Multiply([vgg_model, lstm_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "#     fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "#         metrics=['accuracy'])\n",
    "#     return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fLafkcob0r-"
   },
   "source": [
    "### CREATE QUESTION DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u3mm5ASDb0sI"
   },
   "outputs": [],
   "source": [
    "# create 2 lists\n",
    "# one stores image ids\n",
    "# one stores question\n",
    "\n",
    "imageNamesX  = []\n",
    "questionsNLX = []\n",
    "answers = []\n",
    "\n",
    "import json\n",
    "QAs = json.load(open(\"Training/Quest_Answers.json\", 'r'))['quest_answers']\n",
    "\n",
    "for QA in QAs:\n",
    "    img_name = QA[\"Image\"]+\".png\"\n",
    "    ques = QA[\"Question\"]\n",
    "    ans = QA[\"Answer\"]\n",
    "    \n",
    "    if img_name not in image_dict:\n",
    "        print(\"Skipping {} - not found in dict\".format(img))\n",
    "        continue\n",
    "    \n",
    "    imageNamesX.append(img_name)\n",
    "    questionsNLX.append(ques)\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14572,
     "status": "ok",
     "timestamp": 1542606458156,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "ei2YErUhb0sm",
    "outputId": "91c82222-0433-4c88-df17-09764f9b5de4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 unique tokens.\n",
      "Saving word_index.pickle\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "### NOW do word embeddings for questions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 81\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(questionsNLX)\n",
    "questionsX = tokenizer.texts_to_sequences(questionsNLX)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_length_of_text = 200\n",
    "questionsX = pad_sequences(questionsX, maxlen=max_length_of_text)\n",
    "\n",
    "print(\"Saving word_index.pickle\")\n",
    "import pickle\n",
    "with open('word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10340,
     "status": "ok",
     "timestamp": 1542606458169,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "nVxDXfWWb0s7",
    "outputId": "4e859ee8-6166-4553-b8de-9137276a5049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Embedding Matrix Pickle found...\n",
      ">>> loaded!\n"
     ]
    }
   ],
   "source": [
    "# vector embeddings\n",
    "\n",
    "embeddings_index = {}\n",
    "    \n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embedding_matrix = None\n",
    "if(os.path.isfile('embedding_matrix.pickle')):\n",
    "    print(\">> Embedding Matrix Pickle found...\")\n",
    "    with open('embedding_matrix.pickle', 'rb') as handle:\n",
    "        embedding_matrix = pickle.load(handle)\n",
    "    print(\">>> loaded!\")\n",
    "else:\n",
    "    f = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    with open('embedding_matrix.pickle', 'wb') as handle:\n",
    "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('embedding_matrix.pickle saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3370,
     "status": "ok",
     "timestamp": 1542606458730,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "kAPBVfODb0tX",
    "outputId": "7ec2e9f2-62d7-4da1-faf5-ac9b3d58bce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving label_encoder.pickle\n",
      "label_encoder.pickle saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', 'False', 'True',\n",
       "       'blue', 'brown', 'cube', 'cyan', 'cylinder', 'gray', 'green',\n",
       "       'large', 'metal', 'purple', 'red', 'rubber', 'small', 'sphere',\n",
       "       'yellow'], dtype='<U21')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode answers\n",
    "\n",
    "## ONE HOT\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = answers\n",
    "values = array(data)\n",
    "# print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "# print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "Y = onehot_encoded\n",
    "\n",
    "print(\"Saving label_encoder.pickle\")\n",
    "import pickle\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('label_encoder.pickle saved')\n",
    "\n",
    "# print(onehot_encoded)\n",
    "# invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "def decode_predictions(label_encoder, predictions):\n",
    "    texts = []\n",
    "    for p in predictions:\n",
    "        text = label_encoder.inverse_transform(argmax(p))\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7gwJ6iVo8LMS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split quesionsX\n",
    "from sklearn.model_selection import train_test_split\n",
    "imageNamesX_train, imageNamesX_test, questionsX_train, questionsX_test, Y_train, Y_test = train_test_split(imageNamesX, questionsX, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyd0udLIb0tj"
   },
   "source": [
    "### Data generator ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Oi9ICY1Vb0to"
   },
   "outputs": [],
   "source": [
    "def generator(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "    q_ptr = 0\n",
    "    while True:\n",
    "        image_inp = []\n",
    "        q_inp = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            if q_ptr == len(questions):\n",
    "                q_ptr = 0\n",
    "            index = q_ptr\n",
    "#             import random\n",
    "#             index= random.randint(0, len(questions)-1)\n",
    "            # print(imageNamesX[q_ptr].shape)\n",
    "            # print(questionsX[q_ptr])\n",
    "            image_inp.append(image_dict[img_names[index]])\n",
    "            q_inp.append(questions[index])\n",
    "            batch_labels.append(labels[index])\n",
    "            q_ptr+=1\n",
    "            \n",
    "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KtkNmGDQJQD9"
   },
   "outputs": [],
   "source": [
    "def time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None, timesteps=None):\n",
    "    '''Apply y.w + b for every temporal slice y of x.\n",
    "    '''\n",
    "    if not input_dim:\n",
    "        # won't work with TensorFlow\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        # won't work with TensorFlow\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        # won't work with TensorFlow\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x *= expanded_dropout_matrix\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "\n",
    "    x = K.dot(x, w)\n",
    "    if b:\n",
    "        x = x + b\n",
    "    # reshape to 3D tensor\n",
    "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s9w-mb30esbv"
   },
   "outputs": [],
   "source": [
    "# def generator2(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "#     q_ptr = 0\n",
    "#     while True:\n",
    "#         image_inp = []\n",
    "#         q_inp = []\n",
    "#         batch_labels = []\n",
    "#         for i in range(batch_size):\n",
    "#             if q_ptr == len(questions):\n",
    "#                 q_ptr = 0\n",
    "#             index = q_ptr\n",
    "#             # print(imgsX2[q_ptr].shape)\n",
    "#             # print(quesX2_sq[q_ptr])\n",
    "#             image_inp.append(image_dict[img_names[index]])\n",
    "#             q_inp.append(questions[index])\n",
    "#             batch_labels.append(Y[index])\n",
    "#             q_ptr+=1\n",
    "#         yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WSZV3-5eIzwX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yCjn7TT1esHI"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import keras\n",
    "\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    \n",
    "    print(\"Creating image model...\")\n",
    "    img_model = Sequential()\n",
    "    img_model.add(Dense(1024, input_dim=4096, activation='relu'))\n",
    "\n",
    "    image_input = Input(shape=(4096, ))\n",
    "    encoded_image = img_model(image_input)\n",
    "\n",
    "    print(\"Creating text model...\")\n",
    "    txt_model = Sequential()\n",
    "    txt_model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    txt_model.add(LSTM(units=256, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
    "    txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
    "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
    "#     txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(Dense(1024, activation='tanh'))\n",
    "    \n",
    "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
    "    embedded_question = txt_model(question_input)\n",
    "    \n",
    "    print(txt_model.summary())\n",
    "    \n",
    "    print(\"Merging final model...\")\n",
    "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
    "    d1  = Dense(1024, activation='relu')(merged)\n",
    "    dp1 = Dropout(dropout_rate)(d1)\n",
    "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
    "#     dp2 = Dropout(dropout_rate)(d2)\n",
    "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
    "    \n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    \n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "Yx5BNs_mer_W",
    "outputId": "1c046340-58f0-4222-a3c1-34bb24c61c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "Creating text model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 200)          16200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 484,168\n",
      "Trainable params: 467,968\n",
      "Non-trainable params: 16,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Merging final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=214.317460..., steps_per_epoch=1928.85714..., callbacks=[<keras.ca..., epochs=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 628/1928 [========>.....................] - ETA: 14:14 - loss: 1.3582 - acc: 0.3838"
     ]
    }
   ],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "ckpt_model_weights_filename = \"checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 64\n",
    "model.fit_generator(\n",
    "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8vghhGxL80c3"
   },
   "outputs": [],
   "source": [
    "  #BEST\n",
    "def vqa_model0(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model0\n",
    "    model = vqa_model0.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UkOs6DOA849a"
   },
   "outputs": [],
   "source": [
    "def vqa_model1(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model1\n",
    "    model = vqa_model1.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ujud3WpS86yD"
   },
   "outputs": [],
   "source": [
    "#ANet Experiment\n",
    "def vqa_model3(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model3\n",
    "    model = vqa_model3.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KQsT0kjd9AYc"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weightsV2.bkp\"\n",
    "ckpt_model_weights_filename = \"checkpointsV2/checkP.cp\"\n",
    "\n",
    "model = None\n",
    "if os.path.exists(ckpt_model_weights_filename):\n",
    "    print(\"Loading model: {}\".format(ckpt_model_weights_filename))\n",
    "    from keras.models import load_model\n",
    "    model = load_model(ckpt_model_weights_filename)\n",
    "else:\n",
    "    model = vqa_model0(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print (\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1)\n",
    "\n",
    "batch_size = 10\n",
    "model.fit_generator(\n",
    "    generator(image_dict_V2, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict_V2, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ewHog7aKqS5t"
   },
   "outputs": [],
   "source": [
    "def lstm(sequence_length, text_in):\n",
    "  #text_in = Input(shape = (sequence_length,), name = 'text_in')\n",
    "  text_in = text_in\n",
    "  emb = Embedding(output_dim=256, input_dim=1024, input_length=64)(text_in)\n",
    "  emb = LSTM(256)(emb)\n",
    "  emb = Dropout(0.5)(emb)\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sgGE4b6nqxJv"
   },
   "outputs": [],
   "source": [
    "lstm = lstm(sequence_length, text_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "la7dvNsJqw7G"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import keras\n",
    "\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    \n",
    "    print(\"Creating image model...\")\n",
    "    img_model = Sequential()\n",
    "    img_model.add(Dense(256, input_dim=4096, activation='tanh'))\n",
    "\n",
    "    image_input = Input(shape=(4096, ))\n",
    "    encoded_image = img_model(image_input)\n",
    "\n",
    "    print(\"Creating text model...\")\n",
    "    txt_model = Sequential()\n",
    "    txt_model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    txt_model.add(LSTM(units=256, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
    "    txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
    "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
    "#     txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(Dense(1024, activation='tanh'))\n",
    "    \n",
    "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
    "    embedded_question = txt_model(question_input)\n",
    "    \n",
    "    print(txt_model.summary())\n",
    "    \n",
    "    print(\"Merging final model...\")\n",
    "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
    "    d1  = Dense(1000, activation='softmax')(merged)\n",
    "    dp1 = Dropout(dropout_rate)(d1)\n",
    "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
    "#     dp2 = Dropout(dropout_rate)(d2)\n",
    "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
    "    \n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    \n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    vqa_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ans2Ov84qwt-"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "#ckpt_model_weights_filename = \"checkpoints/checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 128\n",
    "model.fit_generator(\n",
    "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=3, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_or_h1V4b0uc"
   },
   "outputs": [],
   "source": [
    "## EXPERIMENT\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels = [\"hello\", \"world\", \"hello\", \"cat\"]\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Azu2U5Pbb0ux"
   },
   "outputs": [],
   "source": [
    "## ONE HOT\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7m85lTjlb0vB"
   },
   "outputs": [],
   "source": [
    "#BEST\n",
    "def vqa_model0(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model0\n",
    "    model = vqa_model0.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2f_B1QVMb0uW"
   },
   "source": [
    "### EXPERIMENTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gmqJ99xDb0vT"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weightsV2.bkp\"\n",
    "ckpt_model_weights_filename = \"checkpointsV2/checkP.cp\"\n",
    "\n",
    "model = None\n",
    "if os.path.exists(ckpt_model_weights_filename):\n",
    "    print(\"Loading model: {}\".format(ckpt_model_weights_filename))\n",
    "    from keras.models import load_model\n",
    "    model = load_model(ckpt_model_weights_filename)\n",
    "else:\n",
    "    model = vqa_model0(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print (\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k4Ak8p-geGHQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AllScratch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
