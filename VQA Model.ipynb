{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21613,
     "status": "ok",
     "timestamp": 1542606419313,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "Gx32RFZ4cCml",
    "outputId": "bf1b2f54-f1d7-483f-8a7f-d7e7fd97709b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6936,
     "status": "ok",
     "timestamp": 1542606427870,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "d8HRZu8_cdKo",
    "outputId": "e481b0be-95bb-49dd-9e04-82f9dd90fdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is B6F7-1CD2\n",
      "\n",
      " Directory of C:\\Users\\Anirudh\\NNFLproj\n",
      "\n",
      "11/24/2018  12:12 AM    <DIR>          .\n",
      "11/24/2018  12:12 AM    <DIR>          ..\n",
      "11/24/2018  12:12 AM    <DIR>          .ipynb_checkpoints\n",
      "11/24/2018  12:12 AM            60,906 AllScratch(1).ipynb\n",
      "11/19/2018  11:51 PM    <DIR>          clevr-iep-master\n",
      "11/18/2018  07:20 PM    <DIR>          images\n",
      "11/04/2018  05:55 PM        21,006,448 Quest_Answers.json\n",
      "11/20/2018  12:36 AM                91 splitting.py\n",
      "11/18/2018  07:32 PM    11,867,228,256 train_features.h5\n",
      "11/19/2018  11:52 PM        25,928,032 train_questions.h5\n",
      "11/19/2018  11:52 PM             1,613 vocab.json\n",
      "               6 File(s) 11,914,225,346 bytes\n",
      "               5 Dir(s)  17,517,096,960 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6357,
     "status": "ok",
     "timestamp": 1542606427885,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "5tAfZUJ0cc_C",
    "outputId": "693e6da9-5494-461f-f590-0da0d062176b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive\n"
     ]
    }
   ],
   "source": [
    "cd gdrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7351,
     "status": "ok",
     "timestamp": 1542606431587,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "Vd4jBOWkccrV",
    "outputId": "271ef640-309c-4f87-a510-ff4c1ca4944e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6915,
     "status": "ok",
     "timestamp": 1542606431595,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "wdzWU3Ckccg-",
    "outputId": "1026f648-f285-4e75-8ccd-a5c9a32b447f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive\n"
     ]
    }
   ],
   "source": [
    "cd My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11864,
     "status": "ok",
     "timestamp": 1542606436932,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "eO71ukmccvmN",
    "outputId": "3dc40309-6e63-4c9b-a601-f5572641c499"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11500,
     "status": "ok",
     "timestamp": 1542606436938,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "luFIt6WDc2xT",
    "outputId": "f5537b77-f990-4c28-e000-b1681705614f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/NNFL Project\n"
     ]
    }
   ],
   "source": [
    "cd NNFL\\ Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sZa51jJc2kk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIMsk-A9b0p-"
   },
   "source": [
    "### CREATING IMAGE PRE-MODEL (4096 OUTPUT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8vmU4BnCb0qR"
   },
   "outputs": [],
   "source": [
    "# from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# base_model = VGG16()\n",
    "# # print(base_model.summary())\n",
    "\n",
    "# # get output of penultimate layer\n",
    "# from keras.models import Model\n",
    "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uoXJwlFMb0qz"
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# # # load an image from file\n",
    "# # image = load_img('Coffee-Mug.jpg', target_size=(224, 224))\n",
    "# # # convert the image pixels to a numpy array\n",
    "# # image = img_to_array(image)\n",
    "# # # reshape data for the model\n",
    "# # image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# # # prepare the image for the VGG model\n",
    "# # image = preprocess_input(image)\n",
    "\n",
    "# # # predict the probability across all output classes\n",
    "# # yhat = model.predict(image)\n",
    "# # print(yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7cbYqXqb0rG"
   },
   "source": [
    "### MAKE IMAGE DATA DICT ###\n",
    "##### key: image name\n",
    "##### value: vector of shape [1,4096], output of VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ua5zAKxVb0rO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "50 images processed\n",
      "100 images processed\n",
      "150 images processed\n",
      "200 images processed\n",
      "250 images processed\n",
      "300 images processed\n",
      "350 images processed\n",
      "400 images processed\n",
      "450 images processed\n",
      "500 images processed\n",
      "550 images processed\n",
      "600 images processed\n",
      "650 images processed\n",
      "700 images processed\n",
      "750 images processed\n",
      "800 images processed\n",
      "850 images processed\n",
      "900 images processed\n",
      "950 images processed\n",
      "1000 images processed\n",
      "1050 images processed\n",
      "1100 images processed\n",
      "1150 images processed\n",
      "1200 images processed\n",
      "1250 images processed\n",
      "1300 images processed\n",
      "1350 images processed\n",
      "1400 images processed\n",
      "1450 images processed\n",
      "1500 images processed\n",
      "1550 images processed\n",
      "1600 images processed\n",
      "1650 images processed\n",
      "1700 images processed\n",
      "1750 images processed\n",
      "1800 images processed\n",
      "1850 images processed\n",
      "1900 images processed\n",
      "1950 images processed\n",
      "2000 images processed\n",
      "2050 images processed\n",
      "2100 images processed\n",
      "2150 images processed\n",
      "2200 images processed\n",
      "2250 images processed\n",
      "2300 images processed\n",
      "2350 images processed\n",
      "2400 images processed\n",
      "2450 images processed\n",
      "2500 images processed\n",
      "2550 images processed\n",
      "2600 images processed\n",
      "2650 images processed\n",
      "2700 images processed\n",
      "2750 images processed\n",
      "2800 images processed\n",
      "2850 images processed\n",
      "2900 images processed\n",
      "2950 images processed\n",
      "3000 images processed\n",
      "3050 images processed\n",
      "3100 images processed\n",
      "3150 images processed\n",
      "3200 images processed\n",
      "3250 images processed\n",
      "3300 images processed\n",
      "3350 images processed\n",
      "3400 images processed\n",
      "3450 images processed\n",
      "3500 images processed\n",
      "3550 images processed\n",
      "3600 images processed\n",
      "3650 images processed\n",
      "3700 images processed\n",
      "3750 images processed\n",
      "3800 images processed\n",
      "3850 images processed\n",
      "3900 images processed\n",
      "3950 images processed\n",
      "4000 images processed\n",
      "4050 images processed\n",
      "4100 images processed\n",
      "4150 images processed\n",
      "4200 images processed\n",
      "4250 images processed\n",
      "4300 images processed\n",
      "4350 images processed\n",
      "4400 images processed\n",
      "4450 images processed\n",
      "4500 images processed\n",
      "4550 images processed\n",
      "4600 images processed\n",
      "4650 images processed\n",
      "4700 images processed\n",
      "4750 images processed\n",
      "4800 images processed\n",
      "4850 images processed\n",
      "4900 images processed\n",
      "4950 images processed\n",
      "5000 images processed\n",
      "5050 images processed\n",
      "5100 images processed\n",
      "5150 images processed\n",
      "5200 images processed\n",
      "5250 images processed\n",
      "5300 images processed\n",
      "5350 images processed\n",
      "5400 images processed\n",
      "5450 images processed\n",
      "5500 images processed\n",
      "5550 images processed\n",
      "5600 images processed\n",
      "5650 images processed\n",
      "5700 images processed\n",
      "5750 images processed\n",
      "5800 images processed\n",
      "5850 images processed\n",
      "5900 images processed\n",
      "5950 images processed\n",
      "6000 images processed\n",
      "6050 images processed\n",
      "6100 images processed\n",
      "6150 images processed\n",
      "6200 images processed\n",
      "6250 images processed\n",
      "6300 images processed\n",
      "6350 images processed\n",
      "6400 images processed\n",
      "6450 images processed\n",
      "6500 images processed\n",
      "6550 images processed\n",
      "6600 images processed\n",
      "6650 images processed\n",
      "6700 images processed\n",
      "6750 images processed\n",
      "6800 images processed\n",
      "6850 images processed\n",
      "6900 images processed\n",
      "6950 images processed\n",
      "7000 images processed\n",
      "7050 images processed\n",
      "7100 images processed\n",
      "7150 images processed\n",
      "7200 images processed\n",
      "7250 images processed\n",
      "7300 images processed\n",
      "7350 images processed\n",
      "7400 images processed\n",
      "7450 images processed\n",
      "7500 images processed\n",
      "7550 images processed\n",
      "7600 images processed\n",
      "7650 images processed\n",
      "7700 images processed\n",
      "7750 images processed\n",
      "7800 images processed\n",
      "7850 images processed\n",
      "7900 images processed\n",
      "7950 images processed\n",
      "8000 images processed\n",
      "8050 images processed\n",
      "8100 images processed\n",
      "8150 images processed\n",
      "8200 images processed\n",
      "8250 images processed\n",
      "8300 images processed\n",
      "8350 images processed\n",
      "8400 images processed\n",
      "8450 images processed\n",
      "8500 images processed\n",
      "8550 images processed\n",
      "8600 images processed\n",
      "8650 images processed\n",
      "8700 images processed\n",
      "8750 images processed\n",
      "8800 images processed\n",
      "8850 images processed\n",
      "8900 images processed\n",
      "8950 images processed\n",
      "9000 images processed\n",
      "9050 images processed\n",
      "9100 images processed\n",
      "9150 images processed\n",
      "9200 images processed\n",
      "9250 images processed\n",
      "9300 images processed\n",
      "9350 images processed\n",
      "9400 images processed\n",
      "9450 images processed\n",
      "9500 images processed\n",
      "9550 images processed\n",
      "9600 images processed\n",
      "9650 images processed\n",
      "9700 images processed\n",
      "9750 images processed\n",
      "9800 images processed\n",
      "9850 images processed\n",
      "9900 images processed\n",
      "9950 images processed\n",
      "10000 images processed\n",
      "10050 images processed\n",
      "10100 images processed\n",
      "10150 images processed\n",
      "10200 images processed\n",
      "10250 images processed\n",
      "10300 images processed\n",
      "10350 images processed\n",
      "10400 images processed\n",
      "10450 images processed\n",
      "10500 images processed\n",
      "10550 images processed\n",
      "10600 images processed\n",
      "10650 images processed\n",
      "10700 images processed\n",
      "10750 images processed\n",
      "10800 images processed\n",
      "10850 images processed\n",
      "10900 images processed\n",
      "10950 images processed\n",
      "11000 images processed\n",
      "11050 images processed\n",
      "11100 images processed\n",
      "11150 images processed\n",
      "11200 images processed\n",
      "11250 images processed\n",
      "11300 images processed\n",
      "11350 images processed\n",
      "11400 images processed\n",
      "11450 images processed\n",
      "11500 images processed\n",
      "11550 images processed\n",
      "11600 images processed\n",
      "11650 images processed\n",
      "11700 images processed\n",
      "11750 images processed\n",
      "11800 images processed\n",
      "11850 images processed\n",
      "11900 images processed\n",
      "11950 images processed\n",
      "12000 images processed\n",
      "12050 images processed\n",
      "12100 images processed\n",
      "12150 images processed\n",
      "12200 images processed\n",
      "12250 images processed\n",
      "12300 images processed\n",
      "12350 images processed\n",
      "12400 images processed\n",
      "12450 images processed\n",
      "12500 images processed\n",
      "12550 images processed\n",
      "12600 images processed\n",
      "12650 images processed\n",
      "12700 images processed\n",
      "12750 images processed\n",
      "12800 images processed\n",
      "12850 images processed\n",
      "12900 images processed\n",
      "12950 images processed\n",
      "13000 images processed\n",
      "13050 images processed\n",
      "13100 images processed\n",
      "13150 images processed\n",
      "13200 images processed\n",
      "13250 images processed\n",
      "13300 images processed\n",
      "13350 images processed\n",
      "13400 images processed\n",
      "13450 images processed\n",
      "13500 images processed\n",
      "13550 images processed\n",
      "13600 images processed\n",
      "13650 images processed\n",
      "13700 images processed\n",
      "13750 images processed\n",
      "13800 images processed\n",
      "13850 images processed\n",
      "13900 images processed\n",
      "13950 images processed\n",
      "14000 images processed\n",
      "14050 images processed\n",
      "14100 images processed\n",
      "14150 images processed\n",
      "14200 images processed\n",
      "14250 images processed\n",
      "14300 images processed\n",
      "14350 images processed\n",
      "14400 images processed\n",
      "14450 images processed\n",
      "14500 images processed\n",
      "14550 images processed\n",
      "14600 images processed\n",
      "14650 images processed\n",
      "14700 images processed\n",
      "14750 images processed\n",
      "0 files skipped:\n",
      "dict created\n",
      "Saving image_dict_V2.pickle\n",
      "image_dict_V2.pickle saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# imagedictv2: Images of size 100x100\n",
    "\n",
    "if(os.path.isfile('image_dict.pickle')):\n",
    "    with open('image_dict.pickle', 'rb') as handle:\n",
    "        image_dict = pickle.load(handle)\n",
    "        \n",
    "else:\n",
    "  folder = 'images/'\n",
    "\n",
    "  image_dict_V2 = {}        \n",
    "\n",
    "  skipped = []\n",
    "  i=0\n",
    "\n",
    "  for filename in os.listdir(folder):\n",
    "      if(i%50 == 0):\n",
    "          print(\"{} images processed\".format(i))\n",
    "      i+=1\n",
    "      if filename in image_dict_V2:\n",
    "          # print (\"already in dict - moving on\")\n",
    "          continue\n",
    "      try:\n",
    "          # load an image from file\n",
    "          image = cv2.imread(os.path.join(folder, filename))\n",
    "      except:\n",
    "          print(\"Error reading file: {}!!!\".format(filename))\n",
    "          skipped.append(filename)\n",
    "          continue\n",
    "      if image is not None:\n",
    "          resized_image = cv2.resize(image, (100, 100)) \n",
    "          image_dict_V2[filename] = resized_image\n",
    "      else:\n",
    "          skipped.append(filename)\n",
    "\n",
    "  print(\"{} files skipped:\".format(len(skipped)))\n",
    "  for f in skipped:\n",
    "      print(\"    {}\".format(f))\n",
    "  print(\"dict created\")\n",
    "\n",
    "# # Save dict pickle\n",
    "\n",
    "  print(\"Saving image_dict_V2.pickle\")\n",
    "  with open('image_dict_V2.pickle', 'wb') as handle:\n",
    "      pickle.dump(image_dict_V2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      print('image_dict_V2.pickle saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyPHry89b0rg"
   },
   "outputs": [],
   "source": [
    "# # Save dict pickle\n",
    "# import pickle\n",
    "\n",
    "# with open('image_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(images, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     print('image_dict.pickle saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJzDSB7Ob0ru"
   },
   "source": [
    "### COMBINING MODELS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIMeF19ob0r0"
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "# def img_model(dropout_rate):\n",
    "#     print(\"Creating image model...\")\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "#     return model\n",
    "\n",
    "# def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "#     print(\"Creating text model...\")\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(num_words, embedding_dim, \n",
    "#         weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "#     model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(LSTM(units=512, return_sequences=False))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1024, activation='tanh'))\n",
    "#     return model\n",
    "\n",
    "# def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "#     vgg_model = img_model(dropout_rate)\n",
    "#     lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "#     print(\"Merging final model...\")\n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Multiply([vgg_model, lstm_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "#     fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "#         metrics=['accuracy'])\n",
    "#     return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fLafkcob0r-"
   },
   "source": [
    "### CREATE QUESTION DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3mm5ASDb0sI"
   },
   "outputs": [],
   "source": [
    "# create 2 lists\n",
    "# one stores image ids\n",
    "# one stores question\n",
    "\n",
    "imageNamesX  = []\n",
    "questionsNLX = []\n",
    "answers = []\n",
    "\n",
    "import json\n",
    "QAs = json.load(open(\"Quest_Answers.json\", 'r'))['quest_answers']\n",
    "\n",
    "for QA in QAs:\n",
    "    img_name = QA[\"Image\"]+\".png\"\n",
    "    ques = QA[\"Question\"]\n",
    "    ans = QA[\"Answer\"]\n",
    "    \n",
    "    if img_name not in image_dict_V2:\n",
    "        print(\"Skipping {} - not found in dict\".format(img))\n",
    "        continue\n",
    "    \n",
    "    imageNamesX.append(img_name)\n",
    "    questionsNLX.append(ques)\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14572,
     "status": "ok",
     "timestamp": 1542606458156,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "ei2YErUhb0sm",
    "outputId": "91c82222-0433-4c88-df17-09764f9b5de4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 unique tokens.\n",
      "Saving word_index.pickle\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "### NOW do word embeddings for questions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 81\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(questionsNLX)\n",
    "questionsX = tokenizer.texts_to_sequences(questionsNLX)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_length_of_text = 200\n",
    "questionsX = pad_sequences(questionsX, maxlen=max_length_of_text)\n",
    "\n",
    "print(\"Saving word_index.pickle\")\n",
    "import pickle\n",
    "with open('word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10340,
     "status": "ok",
     "timestamp": 1542606458169,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "nVxDXfWWb0s7",
    "outputId": "4e859ee8-6166-4553-b8de-9137276a5049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embedding_matrix.pickle saved\n"
     ]
    }
   ],
   "source": [
    "# vector embeddings\n",
    "\n",
    "embeddings_index = {}\n",
    "    \n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embedding_matrix = None\n",
    "if(os.path.isfile('embedding_matrix.pickle')):\n",
    "    print(\">> Embedding Matrix Pickle found...\")\n",
    "    with open('embedding_matrix.pickle', 'rb') as handle:\n",
    "        embedding_matrix = pickle.load(handle)\n",
    "    print(\">>> loaded!\")\n",
    "else:\n",
    "    f = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    with open('embedding_matrix.pickle', 'wb') as handle:\n",
    "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('embedding_matrix.pickle saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3370,
     "status": "ok",
     "timestamp": 1542606458730,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "kAPBVfODb0tX",
    "outputId": "7ec2e9f2-62d7-4da1-faf5-ac9b3d58bce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving label_encoder.pickle\n",
      "label_encoder.pickle saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', 'False', 'True',\n",
       "       'blue', 'brown', 'cube', 'cyan', 'cylinder', 'gray', 'green',\n",
       "       'large', 'metal', 'purple', 'red', 'rubber', 'small', 'sphere',\n",
       "       'yellow'], dtype='<U11')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode answers\n",
    "\n",
    "## ONE HOT\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = answers\n",
    "values = array(data)\n",
    "# print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "# print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "Y = onehot_encoded\n",
    "\n",
    "print(\"Saving label_encoder.pickle\")\n",
    "import pickle\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('label_encoder.pickle saved')\n",
    "\n",
    "# print(onehot_encoded)\n",
    "# invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "def decode_predictions(label_encoder, predictions):\n",
    "    texts = []\n",
    "    for p in predictions:\n",
    "        text = label_encoder.inverse_transform(argmax(p))\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gwJ6iVo8LMS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split quesionsX\n",
    "from sklearn.model_selection import train_test_split\n",
    "imageNamesX_train, imageNamesX_test, questionsX_train, questionsX_test, Y_train, Y_test = train_test_split(imageNamesX, questionsX, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyd0udLIb0tj"
   },
   "source": [
    "### Data generator ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oi9ICY1Vb0to"
   },
   "outputs": [],
   "source": [
    "def generator(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "    q_ptr = 0\n",
    "    while True:\n",
    "        image_inp = []\n",
    "        q_inp = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            if q_ptr == len(questions):\n",
    "                q_ptr = 0\n",
    "            index = q_ptr\n",
    "#             import random\n",
    "#             index= random.randint(0, len(questions)-1)\n",
    "            # print(imageNamesX[q_ptr].shape)\n",
    "            # print(questionsX[q_ptr])\n",
    "            image_inp.append(image_dict[img_names[index]])\n",
    "            q_inp.append(questions[index])\n",
    "            batch_labels.append(labels[index])\n",
    "            q_ptr+=1\n",
    "            \n",
    "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtkNmGDQJQD9"
   },
   "outputs": [],
   "source": [
    "def time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None, timesteps=None):\n",
    "    '''Apply y.w + b for every temporal slice y of x.\n",
    "    '''\n",
    "    if not input_dim:\n",
    "        # won't work with TensorFlow\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        # won't work with TensorFlow\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        # won't work with TensorFlow\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x *= expanded_dropout_matrix\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "\n",
    "    x = K.dot(x, w)\n",
    "    if b:\n",
    "        x = x + b\n",
    "    # reshape to 3D tensor\n",
    "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s9w-mb30esbv"
   },
   "outputs": [],
   "source": [
    "# def generator2(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "#     q_ptr = 0\n",
    "#     while True:\n",
    "#         image_inp = []\n",
    "#         q_inp = []\n",
    "#         batch_labels = []\n",
    "#         for i in range(batch_size):\n",
    "#             if q_ptr == len(questions):\n",
    "#                 q_ptr = 0\n",
    "#             index = q_ptr\n",
    "#             # print(imgsX2[q_ptr].shape)\n",
    "#             # print(quesX2_sq[q_ptr])\n",
    "#             image_inp.append(image_dict[img_names[index]])\n",
    "#             q_inp.append(questions[index])\n",
    "#             batch_labels.append(Y[index])\n",
    "#             q_ptr+=1\n",
    "#         yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSZV3-5eIzwX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCjn7TT1esHI"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import keras\n",
    "\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    \n",
    "    print(\"Creating image model...\")\n",
    "    img_model = Sequential()\n",
    "    img_model.add(Dense(1024, input_dim=4096, activation='relu'))\n",
    "\n",
    "    image_input = Input(shape=(4096, ))\n",
    "    encoded_image = img_model(image_input)\n",
    "\n",
    "    print(\"Creating text model...\")\n",
    "    txt_model = Sequential()\n",
    "    txt_model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    txt_model.add(LSTM(units=256, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
    "    txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
    "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
    "#     txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(Dense(1024, activation='tanh'))\n",
    "    \n",
    "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
    "    embedded_question = txt_model(question_input)\n",
    "    \n",
    "    print(txt_model.summary())\n",
    "    \n",
    "    print(\"Merging final model...\")\n",
    "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
    "    d1  = Dense(1024, activation='relu')(merged)\n",
    "    dp1 = Dropout(dropout_rate)(d1)\n",
    "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
    "#     dp2 = Dropout(dropout_rate)(d2)\n",
    "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
    "    \n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    \n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "Yx5BNs_mer_W",
    "outputId": "1c046340-58f0-4222-a3c1-34bb24c61c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "Creating text model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 200)          16200     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 484,168\n",
      "Trainable params: 467,968\n",
      "Non-trainable params: 16,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Merging final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=214.317460..., steps_per_epoch=1928.85714..., callbacks=[<keras.ca..., epochs=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_5 to have 2 dimensions, but got array with shape (64, 100, 100, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e4cda9f010ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dict_V2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageNamesX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestionsX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dict_V2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageNamesX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestionsX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=2, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1207\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1209\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_5 to have 2 dimensions, but got array with shape (64, 100, 100, 3)"
     ]
    }
   ],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "ckpt_model_weights_filename = \"checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 64\n",
    "model.fit_generator(\n",
    "    generator(image_dict_V2, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict_V2, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vghhGxL80c3"
   },
   "outputs": [],
   "source": [
    "  #BEST\n",
    "def vqa_model0(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model0\n",
    "    model = vqa_model0.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkOs6DOA849a"
   },
   "outputs": [],
   "source": [
    "def vqa_model1(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model1\n",
    "    model = vqa_model1.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujud3WpS86yD"
   },
   "outputs": [],
   "source": [
    "#ANet Experiment\n",
    "def vqa_model3(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model3\n",
    "    model = vqa_model3.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQsT0kjd9AYc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vqa_model0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a8f7a16a472b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_model_weights_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvqa_model0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_of_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading Weights...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-abd55d4576d9>\u001b[0m in \u001b[0;36mvqa_model0\u001b[1;34m(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#BEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvqa_model0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mvqa_model0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvqa_model0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vqa_model0'"
     ]
    }
   ],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weightsV2.bkp\"\n",
    "ckpt_model_weights_filename = \"checkpointsV2/checkP.cp\"\n",
    "\n",
    "model = None\n",
    "if os.path.exists(ckpt_model_weights_filename):\n",
    "    print(\"Loading model: {}\".format(ckpt_model_weights_filename))\n",
    "    from keras.models import load_model\n",
    "    model = load_model(ckpt_model_weights_filename)\n",
    "else:\n",
    "    model = vqa_model0(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print (\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1)\n",
    "\n",
    "batch_size = 10\n",
    "model.fit_generator(\n",
    "    generator(image_dict_V2, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict_V2, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ewHog7aKqS5t"
   },
   "outputs": [],
   "source": [
    "def lstm(sequence_length, text_in):\n",
    "  #text_in = Input(shape = (sequence_length,), name = 'text_in')\n",
    "  text_in = text_in\n",
    "  emb = Embedding(output_dim=256, input_dim=1024, input_length=64)(text_in)\n",
    "  emb = LSTM(256)(emb)\n",
    "  emb = Dropout(0.5)(emb)\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sgGE4b6nqxJv"
   },
   "outputs": [],
   "source": [
    "lstm = lstm(sequence_length, text_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "la7dvNsJqw7G"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import keras\n",
    "\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    \n",
    "    print(\"Creating image model...\")\n",
    "    img_model = Sequential()\n",
    "    img_model.add(Dense(256, input_dim=4096, activation='tanh'))\n",
    "\n",
    "    image_input = Input(shape=(4096, ))\n",
    "    encoded_image = img_model(image_input)\n",
    "\n",
    "    print(\"Creating text model...\")\n",
    "    txt_model = Sequential()\n",
    "    txt_model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    txt_model.add(LSTM(units=256, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
    "    txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
    "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
    "#     txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(Dense(1024, activation='tanh'))\n",
    "    \n",
    "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
    "    embedded_question = txt_model(question_input)\n",
    "    \n",
    "    print(txt_model.summary())\n",
    "    \n",
    "    print(\"Merging final model...\")\n",
    "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
    "    d1  = Dense(1000, activation='softmax')(merged)\n",
    "    dp1 = Dropout(dropout_rate)(d1)\n",
    "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
    "#     dp2 = Dropout(dropout_rate)(d2)\n",
    "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
    "    \n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    \n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    vqa_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ans2Ov84qwt-"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "#ckpt_model_weights_filename = \"checkpoints/checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 128\n",
    "model.fit_generator(\n",
    "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=3, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_or_h1V4b0uc"
   },
   "outputs": [],
   "source": [
    "## EXPERIMENT\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels = [\"hello\", \"world\", \"hello\", \"cat\"]\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Azu2U5Pbb0ux"
   },
   "outputs": [],
   "source": [
    "## ONE HOT\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7m85lTjlb0vB"
   },
   "outputs": [],
   "source": [
    "#BEST\n",
    "def vqa_model0(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    import vqa_model0\n",
    "    model = vqa_model0.get_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2f_B1QVMb0uW"
   },
   "source": [
    "### EXPERIMENTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gmqJ99xDb0vT"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weightsV2.bkp\"\n",
    "ckpt_model_weights_filename = \"checkpointsV2/checkP.cp\"\n",
    "\n",
    "model = None\n",
    "if os.path.exists(ckpt_model_weights_filename):\n",
    "    print(\"Loading model: {}\".format(ckpt_model_weights_filename))\n",
    "    from keras.models import load_model\n",
    "    model = load_model(ckpt_model_weights_filename)\n",
    "else:\n",
    "    model = vqa_model0(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print (\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k4Ak8p-geGHQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AllScratch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
