{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7cbYqXqb0rG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. MAKE IMAGE DATA DICT\n",
    "\n",
    "* key: image name\n",
    "* Create a folder images with all the required data\n",
    "* Create a image_dict_V2.pickle file for further useage\n",
    "* value: vector of shape [1,4096], output of VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ua5zAKxVb0rO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "50 images processed\n",
      "100 images processed\n",
      "150 images processed\n",
      "200 images processed\n",
      "250 images processed\n",
      "300 images processed\n",
      "350 images processed\n",
      "400 images processed\n",
      "450 images processed\n",
      "500 images processed\n",
      "550 images processed\n",
      "600 images processed\n",
      "650 images processed\n",
      "700 images processed\n",
      "750 images processed\n",
      "800 images processed\n",
      "850 images processed\n",
      "900 images processed\n",
      "950 images processed\n",
      "1000 images processed\n",
      "1050 images processed\n",
      "1100 images processed\n",
      "1150 images processed\n",
      "1200 images processed\n",
      "1250 images processed\n",
      "1300 images processed\n",
      "1350 images processed\n",
      "1400 images processed\n",
      "1450 images processed\n",
      "1500 images processed\n",
      "1550 images processed\n",
      "1600 images processed\n",
      "1650 images processed\n",
      "1700 images processed\n",
      "1750 images processed\n",
      "1800 images processed\n",
      "1850 images processed\n",
      "1900 images processed\n",
      "1950 images processed\n",
      "2000 images processed\n",
      "2050 images processed\n",
      "2100 images processed\n",
      "2150 images processed\n",
      "2200 images processed\n",
      "2250 images processed\n",
      "2300 images processed\n",
      "2350 images processed\n",
      "2400 images processed\n",
      "2450 images processed\n",
      "2500 images processed\n",
      "2550 images processed\n",
      "2600 images processed\n",
      "2650 images processed\n",
      "2700 images processed\n",
      "2750 images processed\n",
      "2800 images processed\n",
      "2850 images processed\n",
      "2900 images processed\n",
      "2950 images processed\n",
      "3000 images processed\n",
      "3050 images processed\n",
      "3100 images processed\n",
      "3150 images processed\n",
      "3200 images processed\n",
      "3250 images processed\n",
      "3300 images processed\n",
      "3350 images processed\n",
      "3400 images processed\n",
      "3450 images processed\n",
      "3500 images processed\n",
      "3550 images processed\n",
      "3600 images processed\n",
      "3650 images processed\n",
      "3700 images processed\n",
      "3750 images processed\n",
      "3800 images processed\n",
      "3850 images processed\n",
      "3900 images processed\n",
      "3950 images processed\n",
      "4000 images processed\n",
      "4050 images processed\n",
      "4100 images processed\n",
      "4150 images processed\n",
      "4200 images processed\n",
      "4250 images processed\n",
      "4300 images processed\n",
      "4350 images processed\n",
      "4400 images processed\n",
      "4450 images processed\n",
      "4500 images processed\n",
      "4550 images processed\n",
      "4600 images processed\n",
      "4650 images processed\n",
      "4700 images processed\n",
      "4750 images processed\n",
      "4800 images processed\n",
      "4850 images processed\n",
      "4900 images processed\n",
      "4950 images processed\n",
      "5000 images processed\n",
      "5050 images processed\n",
      "5100 images processed\n",
      "5150 images processed\n",
      "5200 images processed\n",
      "5250 images processed\n",
      "5300 images processed\n",
      "5350 images processed\n",
      "5400 images processed\n",
      "5450 images processed\n",
      "5500 images processed\n",
      "5550 images processed\n",
      "5600 images processed\n",
      "5650 images processed\n",
      "5700 images processed\n",
      "5750 images processed\n",
      "5800 images processed\n",
      "5850 images processed\n",
      "5900 images processed\n",
      "5950 images processed\n",
      "6000 images processed\n",
      "6050 images processed\n",
      "6100 images processed\n",
      "6150 images processed\n",
      "6200 images processed\n",
      "6250 images processed\n",
      "6300 images processed\n",
      "6350 images processed\n",
      "6400 images processed\n",
      "6450 images processed\n",
      "6500 images processed\n",
      "6550 images processed\n",
      "6600 images processed\n",
      "6650 images processed\n",
      "6700 images processed\n",
      "6750 images processed\n",
      "6800 images processed\n",
      "6850 images processed\n",
      "6900 images processed\n",
      "6950 images processed\n",
      "7000 images processed\n",
      "7050 images processed\n",
      "7100 images processed\n",
      "7150 images processed\n",
      "7200 images processed\n",
      "7250 images processed\n",
      "7300 images processed\n",
      "7350 images processed\n",
      "7400 images processed\n",
      "7450 images processed\n",
      "7500 images processed\n",
      "7550 images processed\n",
      "7600 images processed\n",
      "7650 images processed\n",
      "7700 images processed\n",
      "7750 images processed\n",
      "7800 images processed\n",
      "7850 images processed\n",
      "7900 images processed\n",
      "7950 images processed\n",
      "8000 images processed\n",
      "8050 images processed\n",
      "8100 images processed\n",
      "8150 images processed\n",
      "8200 images processed\n",
      "8250 images processed\n",
      "8300 images processed\n",
      "8350 images processed\n",
      "8400 images processed\n",
      "8450 images processed\n",
      "8500 images processed\n",
      "8550 images processed\n",
      "8600 images processed\n",
      "8650 images processed\n",
      "8700 images processed\n",
      "8750 images processed\n",
      "8800 images processed\n",
      "8850 images processed\n",
      "8900 images processed\n",
      "8950 images processed\n",
      "9000 images processed\n",
      "9050 images processed\n",
      "9100 images processed\n",
      "9150 images processed\n",
      "9200 images processed\n",
      "9250 images processed\n",
      "9300 images processed\n",
      "9350 images processed\n",
      "9400 images processed\n",
      "9450 images processed\n",
      "9500 images processed\n",
      "9550 images processed\n",
      "9600 images processed\n",
      "9650 images processed\n",
      "9700 images processed\n",
      "9750 images processed\n",
      "9800 images processed\n",
      "9850 images processed\n",
      "9900 images processed\n",
      "9950 images processed\n",
      "10000 images processed\n",
      "10050 images processed\n",
      "10100 images processed\n",
      "10150 images processed\n",
      "10200 images processed\n",
      "10250 images processed\n",
      "10300 images processed\n",
      "10350 images processed\n",
      "10400 images processed\n",
      "10450 images processed\n",
      "10500 images processed\n",
      "10550 images processed\n",
      "10600 images processed\n",
      "10650 images processed\n",
      "10700 images processed\n",
      "10750 images processed\n",
      "10800 images processed\n",
      "10850 images processed\n",
      "10900 images processed\n",
      "10950 images processed\n",
      "11000 images processed\n",
      "11050 images processed\n",
      "11100 images processed\n",
      "11150 images processed\n",
      "11200 images processed\n",
      "11250 images processed\n",
      "11300 images processed\n",
      "11350 images processed\n",
      "11400 images processed\n",
      "11450 images processed\n",
      "11500 images processed\n",
      "11550 images processed\n",
      "11600 images processed\n",
      "11650 images processed\n",
      "11700 images processed\n",
      "11750 images processed\n",
      "11800 images processed\n",
      "11850 images processed\n",
      "11900 images processed\n",
      "11950 images processed\n",
      "12000 images processed\n",
      "12050 images processed\n",
      "12100 images processed\n",
      "12150 images processed\n",
      "12200 images processed\n",
      "12250 images processed\n",
      "12300 images processed\n",
      "12350 images processed\n",
      "12400 images processed\n",
      "12450 images processed\n",
      "12500 images processed\n",
      "12550 images processed\n",
      "12600 images processed\n",
      "12650 images processed\n",
      "12700 images processed\n",
      "12750 images processed\n",
      "12800 images processed\n",
      "12850 images processed\n",
      "12900 images processed\n",
      "12950 images processed\n",
      "13000 images processed\n",
      "13050 images processed\n",
      "13100 images processed\n",
      "13150 images processed\n",
      "13200 images processed\n",
      "13250 images processed\n",
      "13300 images processed\n",
      "13350 images processed\n",
      "13400 images processed\n",
      "13450 images processed\n",
      "13500 images processed\n",
      "13550 images processed\n",
      "13600 images processed\n",
      "13650 images processed\n",
      "13700 images processed\n",
      "13750 images processed\n",
      "13800 images processed\n",
      "13850 images processed\n",
      "13900 images processed\n",
      "13950 images processed\n",
      "14000 images processed\n",
      "14050 images processed\n",
      "14100 images processed\n",
      "14150 images processed\n",
      "14200 images processed\n",
      "14250 images processed\n",
      "14300 images processed\n",
      "14350 images processed\n",
      "14400 images processed\n",
      "14450 images processed\n",
      "14500 images processed\n",
      "14550 images processed\n",
      "14600 images processed\n",
      "14650 images processed\n",
      "14700 images processed\n",
      "14750 images processed\n",
      "0 files skipped:\n",
      "dict created\n",
      "Saving image_dict_V2.pickle\n",
      "image_dict_V2.pickle saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# imagedictv2: Images of size 100x100\n",
    "\n",
    "if(os.path.isfile('image_dict.pickle')):\n",
    "    with open('image_dict.pickle', 'rb') as handle:\n",
    "        image_dict = pickle.load(handle)\n",
    "        \n",
    "else:\n",
    "  folder = 'images/'\n",
    "\n",
    "  image_dict_V2 = {}        \n",
    "\n",
    "  skipped = []\n",
    "  i=0\n",
    "\n",
    "  for filename in os.listdir(folder):\n",
    "      if(i%500 == 0):\n",
    "          print(\"{} images processed\".format(i))\n",
    "      i+=1\n",
    "      if filename in image_dict_V2:\n",
    "          # print (\"already in dict - moving on\")\n",
    "          continue\n",
    "      try:\n",
    "          # load an image from file\n",
    "          image = cv2.imread(os.path.join(folder, filename))\n",
    "      except:\n",
    "          print(\"Error reading file: {}!!!\".format(filename))\n",
    "          skipped.append(filename)\n",
    "          continue\n",
    "      if image is not None:\n",
    "          resized_image = cv2.resize(image, (100, 100)) \n",
    "          image_dict_V2[filename] = resized_image\n",
    "      else:\n",
    "          skipped.append(filename)\n",
    "\n",
    "  print(\"{} files skipped:\".format(len(skipped)))\n",
    "  for f in skipped:\n",
    "      print(\"    {}\".format(f))\n",
    "  print(\"dict created\")\n",
    "\n",
    "# # Save dict pickle\n",
    "\n",
    "  print(\"Saving image_dict_V2.pickle\")\n",
    "  with open('image_dict_V2.pickle', 'wb') as handle:\n",
    "      pickle.dump(image_dict_V2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      print('image_dict_V2.pickle saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fLafkcob0r-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. CREATE QUESTION DATA \n",
    "\n",
    "* load Quest_Answers.json\n",
    "* store image ID in a list\n",
    "* store question in a list\n",
    "* Store answer in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3mm5ASDb0sI"
   },
   "outputs": [],
   "source": [
    "# create 2 lists\n",
    "# one stores image ids\n",
    "# one stores question\n",
    "\n",
    "imageNamesX  = []\n",
    "questionsNLX = []\n",
    "answers = []\n",
    "\n",
    "import json\n",
    "QAs = json.load(open(\"Quest_Answers.json\", 'r'))['quest_answers']\n",
    "\n",
    "for QA in QAs:\n",
    "    img_name = QA[\"Image\"]+\".png\"\n",
    "    ques = QA[\"Question\"]\n",
    "    ans = QA[\"Answer\"]\n",
    "    \n",
    "    if img_name not in image_dict_V2:\n",
    "        print(\"Skipping {} - not found in dict\".format(img))\n",
    "        continue\n",
    "    \n",
    "    imageNamesX.append(img_name)\n",
    "    questionsNLX.append(ques)\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.0 Create word embeddings\n",
    "\n",
    "* Using the keras.preprocessing module Tokenize the questions\n",
    "* Create a word index from the tokenizer object\n",
    "* Save the word index as a word_index.pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14572,
     "status": "ok",
     "timestamp": 1542606458156,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "ei2YErUhb0sm",
    "outputId": "91c82222-0433-4c88-df17-09764f9b5de4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 unique tokens.\n",
      "Saving word_index.pickle\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "### NOW do word embeddings for questions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 81\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(questionsNLX)\n",
    "questionsX = tokenizer.texts_to_sequences(questionsNLX)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_length_of_text = 200\n",
    "questionsX = pad_sequences(questionsX, maxlen=max_length_of_text)\n",
    "\n",
    "print(\"Saving word_index.pickle\")\n",
    "import pickle\n",
    "with open('word_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.0 Create vector embeddings using Glove\n",
    "\n",
    "* download the glove embedding file from https://www.kaggle.com/incorpes/glove6b200d\n",
    "* Use this file to create the embedding matrix\n",
    "* Save the embedding matrix as embedding_matrix.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10340,
     "status": "ok",
     "timestamp": 1542606458169,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "nVxDXfWWb0s7",
    "outputId": "4e859ee8-6166-4553-b8de-9137276a5049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Embedding Matrix Pickle found...\n",
      ">>> loaded!\n"
     ]
    }
   ],
   "source": [
    "# vector embeddings\n",
    "\n",
    "embeddings_index = {}\n",
    "    \n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embedding_matrix = None\n",
    "if(os.path.isfile('embedding_matrix.pickle')):\n",
    "    print(\">> Embedding Matrix Pickle found...\")\n",
    "    with open('embedding_matrix.pickle', 'rb') as handle:\n",
    "        embedding_matrix = pickle.load(handle)\n",
    "    print(\">>> loaded!\")\n",
    "else:\n",
    "    f = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    with open('embedding_matrix.pickle', 'wb') as handle:\n",
    "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('embedding_matrix.pickle saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 One Hot Encoding of target values\n",
    "\n",
    "* Create a one hot encoding of the answers list created above\n",
    "* save as label_encoder.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3370,
     "status": "ok",
     "timestamp": 1542606458730,
     "user": {
      "displayName": "AVIRAL AGRAWAL",
      "photoUrl": "",
      "userId": "03227679886360719676"
     },
     "user_tz": -330
    },
    "id": "kAPBVfODb0tX",
    "outputId": "7ec2e9f2-62d7-4da1-faf5-ac9b3d58bce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving label_encoder.pickle\n",
      "label_encoder.pickle saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', 'False', 'True',\n",
       "       'blue', 'brown', 'cube', 'cyan', 'cylinder', 'gray', 'green',\n",
       "       'large', 'metal', 'purple', 'red', 'rubber', 'small', 'sphere',\n",
       "       'yellow'], dtype='<U11')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode answers\n",
    "\n",
    "## ONE HOT\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = answers\n",
    "values = array(data)\n",
    "# print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "# print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "Y = onehot_encoded\n",
    "\n",
    "print(\"Saving label_encoder.pickle\")\n",
    "import pickle\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('label_encoder.pickle saved')\n",
    "\n",
    "# print(onehot_encoded)\n",
    "# invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "def decode_predictions(label_encoder, predictions):\n",
    "    texts = []\n",
    "    for p in predictions:\n",
    "        text = label_encoder.inverse_transform(argmax(p))\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6.0 Split the created variables \n",
    "\n",
    "* Create imageNamesX_train\n",
    "* Create imageNamesX_test\n",
    "* Create questionsX_train\n",
    "* Create questionsX_test\n",
    "* Create Y_train\n",
    "* Create Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gwJ6iVo8LMS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split quesionsX\n",
    "from sklearn.model_selection import train_test_split\n",
    "imageNamesX_train, imageNamesX_test, questionsX_train, questionsX_test, Y_train, Y_test = train_test_split(imageNamesX, questionsX, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7.0 Models and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJzDSB7Ob0ru",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7.1 COMBINING MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIMeF19ob0r0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "def img_model(dropout_rate):\n",
    "    print(\"Creating image model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating text model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def vqa_model1(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    fc_model = Sequential()\n",
    "    # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "    fc_model.add(Multiply([vgg_model, lstm_model]))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7.2 Create the VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, Multiply, Concatenate, Conv2D, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import keras\n",
    "\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    \n",
    "    print(\"Creating image model...\")\n",
    "    img_model = Sequential()\n",
    "    img_model.add(Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu'))\n",
    "    img_model.add(BatchNormalization())\n",
    "    img_model.add(Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu'))\n",
    "    img_model.add(BatchNormalization())\n",
    "    img_model.add(Conv2D(48, kernel_size=(3, 3), strides=2, activation='relu'))\n",
    "    img_model.add(BatchNormalization())\n",
    "    img_model.add(Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu'))\n",
    "    img_model.add(BatchNormalization())\n",
    "    img_model.add(keras.layers.Flatten())\n",
    "    \n",
    "    image_input = Input(shape=(100, 100, 3))\n",
    "    encoded_image = img_model(image_input)\n",
    "    \n",
    "    print(img_model.summary())\n",
    "\n",
    "    print(\"Creating text model...\")\n",
    "    txt_model = Sequential()\n",
    "    txt_model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    txt_model.add(LSTM(units=128, return_sequences=False, input_shape=(seq_length, embedding_dim)))\n",
    "    txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(AttentionDecoder(512, embedding_dim))\n",
    "#     txt_model.add(LSTM(units=512, return_sequences=False))\n",
    "#     txt_model.add(Dropout(dropout_rate))\n",
    "#     txt_model.add(Dense(1024, activation='tanh'))\n",
    "    \n",
    "    question_input = Input(shape=(EMBEDDING_DIM, ), dtype='int32')\n",
    "    embedded_question = txt_model(question_input)\n",
    "    \n",
    "    print(txt_model.summary())\n",
    "    \n",
    "    print(\"Merging final model...\")\n",
    "    merged = keras.layers.concatenate([encoded_image, embedded_question])\n",
    "    d1  = Dense(512, activation='relu')(merged)\n",
    "    dp1 = Dropout(dropout_rate)(d1)\n",
    "#     d2  = Dense(1000, activation='tanh')(dp1)\n",
    "#     dp2 = Dropout(dropout_rate)(d2)\n",
    "    output  = Dense(num_classes, activation='softmax')(dp1)\n",
    "    \n",
    "    vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "    \n",
    "    \n",
    "#     fc_model = Sequential()\n",
    "#     # fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "#     fc_model.add(Concatenate([img_model, txt_model]))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(1000, activation='tanh'))\n",
    "#     fc_model.add(Dropout(dropout_rate))\n",
    "#     fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 8.0 Create image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9w-mb30esbv"
   },
   "outputs": [],
   "source": [
    "### Data generator ###\n",
    "\n",
    "def generator(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "    q_ptr = 0\n",
    "    while True:\n",
    "        image_inp = []\n",
    "        q_inp = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            if q_ptr == len(questions):\n",
    "                q_ptr = 0\n",
    "            index = q_ptr\n",
    "#             import random\n",
    "#             index= random.randint(0, len(questions)-1)\n",
    "            # print(imageNamesX[q_ptr].shape)\n",
    "            # print(questionsX[q_ptr])\n",
    "            image_inp.append(image_dict[img_names[index]])\n",
    "            q_inp.append(questions[index])\n",
    "            batch_labels.append(labels[index])\n",
    "            q_ptr+=1\n",
    "            \n",
    "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generator(image_dict, img_names, questions, labels, batch_size):\n",
    "    \n",
    "    q_ptr = 0\n",
    "    while True:\n",
    "        image_inp = []\n",
    "        q_inp = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            if q_ptr == len(questions):\n",
    "                q_ptr = 0\n",
    "            index = q_ptr\n",
    "            # print(imgsX2[q_ptr].shape)\n",
    "            # print(quesX2_sq[q_ptr])\n",
    "            image_inp.append(image_dict[img_names[index]])\n",
    "            q_inp.append(questions[index])\n",
    "            batch_labels.append(Y[index])\n",
    "            q_ptr+=1\n",
    "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 10. Fit thedata to the model \n",
    "\n",
    "* use model.fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "Yx5BNs_mer_W",
    "outputId": "1c046340-58f0-4222-a3c1-34bb24c61c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 49, 49, 24)        672       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 49, 49, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 48)        10416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 48)        20784     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 11, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 64)          27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "=================================================================\n",
      "Total params: 60,320\n",
      "Trainable params: 59,952\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n",
      "Creating text model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 200)          16200     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 184,648\n",
      "Trainable params: 168,448\n",
      "Non-trainable params: 16,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Merging final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Anirudh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=214.317460..., steps_per_epoch=1928.85714..., callbacks=[<keras.ca..., epochs=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  88/1928 [>.............................] - ETA: 7:38:20 - loss: 3.0405 - acc: 0.1772"
     ]
    }
   ],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "ckpt_model_weights_filename = \"checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 64\n",
    "model.fit_generator(\n",
    "    generator(image_dict_V2, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict_V2, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2f_B1QVMb0uW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EXPERIMENTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQsT0kjd9AYc"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weightsV2.bkp\"\n",
    "ckpt_model_weights_filename = \"checkpointsV2/checkP.cp\"\n",
    "\n",
    "model = None\n",
    "if os.path.exists(ckpt_model_weights_filename):\n",
    "    print(\"Loading model: {}\".format(ckpt_model_weights_filename))\n",
    "    from keras.models import load_model\n",
    "    model = load_model(ckpt_model_weights_filename)\n",
    "else:\n",
    "    model = vqa_model0(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print (\"Loading Weights...\")\n",
    "        model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename, verbose=1)\n",
    "\n",
    "batch_size = 10\n",
    "model.fit_generator(\n",
    "    generator(image_dict_V2, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict_V2, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ans2Ov84qwt-"
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.5\n",
    "num_classes=26\n",
    "model_weights_filename = \"weights.bkp\"\n",
    "#ckpt_model_weights_filename = \"checkpoints/checkP.cp\"\n",
    "\n",
    "model = vqa_model(embedding_matrix, num_words, EMBEDDING_DIM, max_length_of_text, dropout_rate, num_classes)\n",
    "if os.path.exists(model_weights_filename):\n",
    "    print (\"Loading Weights...\")\n",
    "    model.load_weights(model_weights_filename)\n",
    "    \n",
    "# print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=ckpt_model_weights_filename,verbose=1)\n",
    "\n",
    "batch_size = 128\n",
    "model.fit_generator(\n",
    "    generator(image_dict, imageNamesX_train, questionsX_train, Y_train, batch_size),\n",
    "    validation_data=generator(image_dict, imageNamesX_test, questionsX_test, Y_test, batch_size),\n",
    "    validation_steps=len(Y_test)/(batch_size-1), steps_per_epoch=len(Y_train)/(batch_size-1), nb_epoch=3, callbacks=[checkpointer])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AllScratch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}